{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # 0 = AAPL\n",
    "    # 1 = MSI\n",
    "    # 2 = SBUX\n",
    "    df = pd.read_csv('aapl_msi_sbux.csv')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros(size, dtype=np.uint8)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.uint8)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "    \n",
    "    def store(self, obs, next_obs, act, rew, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "        \n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(s=self.obs1_buf[idxs], \n",
    "                    s2=self.obs2_buf[idxs], \n",
    "                    a=self.acts_buf[idxs], \n",
    "                    r=self.rews_buf[idxs], \n",
    "                    d=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(env):\n",
    "    # returns scikit-learn scaler object to scale the states\n",
    "    \n",
    "    states = []\n",
    "    for _ in range(env.n_step):\n",
    "        action = np.random.choice(env.action_space)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        states.append(state)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(states)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_make_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_action, n_hidden_layers=1, hidden_dim=32):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        M = n_inputs\n",
    "        for _ in range(n_hidden_layers):\n",
    "            layer = nn.Linear(M, hidden_dim)\n",
    "            M = hidden_dim\n",
    "            self.layers.append(layer)\n",
    "            self.layers.append(nn.ReLU())\n",
    "        # final layer\n",
    "        self.layers.append(nn.Linear(M, n_action))\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "    def save_weights(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, np_states):\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.from_numpy(np_states.astype(np.float32))\n",
    "        output = model(inputs)\n",
    "        return output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, criterion, optimizer, inputs, targets):\n",
    "    # convert to tensors\n",
    "    inputs = torch.from_numpy(inputs.astype(np.float32))\n",
    "    targets = torch.from_numpy(targets.astype(np.float32))\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # backward and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStockEnv():\n",
    "    def __init__(self, data, initial_investment=20000):\n",
    "        # data\n",
    "        self.stock_price_history = data\n",
    "        self.n_step, self.n_stock = self.stock_price_history.shape\n",
    "        \n",
    "        # instance attributes\n",
    "        self.initial_investment = initial_investment\n",
    "        self.current_step = None\n",
    "        self.stock_owned = None\n",
    "        self.stock_price = None\n",
    "        self.cash_in_hand = None\n",
    "        self.action_space = np.arange(3**self.n_stock)\n",
    "        self.action_list = list(map(list, itertools.product([0,1,2], repeat=self.n_stock)))\n",
    "        # calculate size of state\n",
    "        self.state_dim = self.n_stock * 2 + 1\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.stock_owned = np.zeros(self.n_stock)\n",
    "        self.stock_price = self.stock_price_history[self.current_step]\n",
    "        self.cash_in_hand = self.initial_investment\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert action in self.action_space\n",
    "        # get current value before performing the action\n",
    "        prev_val = self._get_val()\n",
    "        # update price, i.e. go to the next day\n",
    "        self.current_step += 1\n",
    "        self.stock_price = self.stock_price_history[self.current_step]\n",
    "        # perform the trade\n",
    "        self._trade(action)\n",
    "        # get the new value after taking the action\n",
    "        current_val = self._get_val()\n",
    "        \n",
    "        # reward is the increase in portfolio value\n",
    "        reward = current_val - prev_val\n",
    "        # done if we have run out of data\n",
    "        done = self.current_step == self.n_step - 1\n",
    "        # store the current value of the portfolio here\n",
    "        info = {'current_value':current_val}\n",
    "        # conform to the Gym API\n",
    "        return self._get_obs(), reward, done, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        obs = np.empty(self.state_dim)\n",
    "        obs[:self.n_stock] = self.stock_owned\n",
    "        obs[self.n_stock:2*self.n_stock] = self.stock_price\n",
    "        obs[-1] = self.cash_in_hand\n",
    "        return obs\n",
    "    \n",
    "    def _get_val(self):\n",
    "        return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
    "    \n",
    "    def _trade(self, action):\n",
    "        action_vec = self.action_list[action]\n",
    "        \n",
    "        sell_index = []\n",
    "        buy_index = []\n",
    "        for i, a in enumerate(action_vec):\n",
    "            if a==0:\n",
    "                sell_index.append(i)\n",
    "            elif a==2:\n",
    "                buy_index.append(i)\n",
    "                \n",
    "        if sell_index:\n",
    "            # NOTE: To simplify the problem, when we sell, we will sell ALL shares of that stock\n",
    "            for i in sell_index:\n",
    "                self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
    "                self.stock_owned[i] = 0\n",
    "        if buy_index:\n",
    "            # NOTE: when buying , we will loop through each stock we want to buy, \n",
    "            # # # # and buy one share at a time until we run out of cash\n",
    "            can_buy = True\n",
    "            while can_buy:\n",
    "                for i in buy_index:\n",
    "                    if self.cash_in_hand > self.stock_price[i]:\n",
    "                        self.stock_owned[i] += 1\n",
    "                        self.cash_in_hand -= self.stock_price[i]\n",
    "                    else:\n",
    "                        can_buy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent is our Artificial Intelligence (AI)\n",
    "class DQNAgent(object):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = ReplayBuffer(state_size, action_size, size=500)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = MLP(state_size, action_size)\n",
    "        # Loss and Optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        \n",
    "    def updatereplay_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.store(state, action, reward, next_state, done)\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = predict(self.model, state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def replay(self, batch_size=32):\n",
    "        # first check if replay buffer contains enough data\n",
    "        if self.memory.size < batch_size:\n",
    "            return\n",
    "        \n",
    "        # sample a batch of data from the replay memory\n",
    "        minibatch = self.memory.sample_batch(batch_size)\n",
    "        states = minibatch['s']\n",
    "        actions = minibatch['a']\n",
    "        rewards = minibatch['r']\n",
    "        next_states = minibatch['s2']\n",
    "        done = minibatch['d']\n",
    "        \n",
    "        # Calculate the Target: Q(s', a)\n",
    "        target = rewards + (1-done) * self.gamma * np.max(self.model.predict(next_states), axis=1)\n",
    "        target_full = predict(self.model, states)\n",
    "        target_full[np.arange(batch_size), actions] = target\n",
    "        \n",
    "        # Run one training step\n",
    "        train_one_step(self.model, self.criterion, self.optimizer, states, target_full)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(agent, env, is_train):\n",
    "    # Note: after transforming states are already 1xD\n",
    "    state = env.reset()\n",
    "    state = scaler.transform([state])\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = scaler.transform(next_state)\n",
    "        if is_train == 'train':\n",
    "            agent.update_replay_memory(state, action, reward, next_state, done)\n",
    "            agent.replay(batch_size)\n",
    "        state = next_state\n",
    "        \n",
    "    return inf['current_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -m/--mode M/__MODE\n",
      "ipykernel_launcher.py: error: the following arguments are required: -m/--mode\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # config\n",
    "    models_folder = 'rl_trader_models'\n",
    "    rewards_folder = 'rl_trader_rewards'\n",
    "    num_episodes = 2000\n",
    "    batch_size = 32\n",
    "    initial_investment = 20000\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-m/--mode', type=str, required=True, help='either \"train\" or \"test\"')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    maybe_make_dir(models_folder)\n",
    "    maybe_make_dir(rewards_folder)\n",
    "    \n",
    "    data = get_data()\n",
    "    n_timesteps, n_stocks = data.shape\n",
    "    n_train = n_timesteps // 2\n",
    "    train_data = data[:n_train]\n",
    "    test_data = data[n_train:]\n",
    "    \n",
    "    env = MultiStockEnv(train_data, initial_investment)\n",
    "    state_size = env.state_dim\n",
    "    action_size = len(env.action_space)\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    scaler = get_scaler(env)\n",
    "    \n",
    "    # store the final value of the portfolio (end of episode)\n",
    "    portfolio_value = []\n",
    "    if args.mode == 'test':\n",
    "        # then load the previous scaler\n",
    "        with open(f'{models_folder}/scaler.pkl', 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        \n",
    "        # remake the env with test data\n",
    "        env = MultiStockEnv(test_data, initial_investment)\n",
    "        \n",
    "        # make sure epsilon is not 1!\n",
    "        # no need to run multiple episodes if epsilon = 0, it is deterministic\n",
    "        agent.epsilon = 0.01\n",
    "        \n",
    "        # load trained weights\n",
    "        agent.load(f'{models_folder}/dqn.h5')\n",
    "    \n",
    "    # play the game num_episodes times\n",
    "    for e in range(num_episodes):\n",
    "        t0 = datetime.now()\n",
    "        val = play_one_episode(agent, env, args.mode)\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'episode: {e+1}/{num_episodes}, episode end value: {val:.2f}, duration: {dt}')\n",
    "        portfolio_value.append(val)  # append episode end portfolio value\n",
    "        \n",
    "    # save the weights when we are done\n",
    "    if args.mode == 'train':\n",
    "        # save the DQN\n",
    "        agent.save(f'{models_folder}/dqn.h5')\n",
    "        \n",
    "        # save the scaler\n",
    "        with open(f'{models_folder}/scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        \n",
    "    # save portfolio value for each episode\n",
    "    np.save(f'{rewards_folder}/{args.mode}.npy', portfolio_value)\n",
    "    \n",
    "# run on cmd\n",
    "# py rl_trader.py -m train\n",
    "# py rl_trader.py -m test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pypot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-m', '--mode', type=str, required=True, help='either \"train\" or \"test\"')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "a = np.load(f'linear_rl_trader_rewards/{args.mode}.npy')\n",
    "\n",
    "print(f'average reward: {a.mean():.2f}, min: {a.min():.2f}, max: {a.max():.2f}')\n",
    "\n",
    "plt.hist(a, bins=20)\n",
    "plt.title(args.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
